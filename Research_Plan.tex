The research plan of this study will follow the work of Peffers et al. described in Section \ref{sec:disMethodology}. Each required step of Peffers et al. will be considered in its own section and how that step applies to the current research will be discussed.

\subsection{Identify, define, and motivate the focal problem}

Our university industry partner has a core service that is pre-employment assessments.
Currently they offer third party organisations an efficient means to bring
candidates on-board which can involve interview(s), medical questionnaires and medical
assessments. Through experience they have realised that sometimes a candidate that appears
perfect for a role fails at the last hurdle of the selection process, being a medical assessment.
It is this late assessment failure that brings rise to the core problem of this research. \textit{How can
    a potential candidate be assessed on some medical criteria without involving an actual medical
    assessment?} A curious reader may wonder why the assessment occurs so late in the process?
This is because the selection process
starts with many multiples of the final number of candidates actually being assessed and so would be prohibitively expensive
to offer the assessments any earlier.


\subsection{Define objectives that a solution (possibly partial) to the focal problem must achieve}

In order to address the focal problem any potential solution should garner useful information from the candidate's answers
to a preselection questionnaire that they are required to complete. The questions contained
within any such questionnaire should take into account the specific role for which the candidate is applying
and any typical risks or needs that are associated with that role.

\subsection{Design and develop the artefact}

Before detailing the artefacts to be developed that correspond to the objectives of this work it would be prudent to give an overview of the proposed system.

\subsubsection{System Overview}

Being able to definitively label a role or job title is critical to the operation of any system being developed.
For instance a bus driver could also be referred to as a bus operator or omnibus driver and so a standard convention
is required. This convention within Australia and New Zealand is the Australian and New Zealand Standard Classification
of Occupations (ANZSCO). This ANZSCO standard will form the "bridge" between the description of a role given by
a third party and the definition of the role within the system. This "bridge" is described in Figure~\ref{fig:anzcoriskprofile}.
Within the bridge exists an ANZSCO semantic search which represents another research opportunity currently being written
within the university. The bridge also contains multiple "risk profile templates" which represent a predefined "risk profile"
enabling third party's to have a building block for their specific role.

\input{fig/Fig_ANZCO_riskprofile}

The artefacts to be developed should be able to categorise candidates for specific roles where each role is
associated with one or more "risk profiles". The purpose of a risk profile is to form the association between
particular job requirements such as "lifting heavy weight from floor to waist" or "sitting for extended periods"
through to the body parts that are affected
due to that requirement, such as back, arm, leg, or shoulder. From these body parts a number of common injuries are linked and from those injuries
ultimately a series of questions are added to the questionnaire for that role. This risk profile association
for an individual role is shown in Figure~\ref{fig:riskprofile}. In this figure we see two job requirements 1 \& 2
that follow the classic association just described whereas job requirement 3 links directly to multiple question groups.
An example of such a requirement would be a bus driver where a particular licence is a requirement which obviously has
no reliance on a particular body part.


\input{fig/Fig_riskprofile}

There are a few complications of the questionnaire that a model must adapt to. The first being its hierarchical or cascading
nature which is best described through an example.
A question could be framed asking "have you ever broken your leg?" the answer a candidate gives could be either yes/no.
If the candidate enters "yes" however this could trigger further questions about frequency, time to heal etc. and each
successive answer could themselves create a hierarchical set of questions. The other complication is the fact that answers
to questions can be of various types, for instance a simple quantity like 4, a category like 'male', a list of possible
answers, a ranking like [0-7] or a linguistic ranking like 'tall'.

Initially categorisation of a candidate will involve training an individual classification model for each role from
the answers collected through the associated questionnaire. This classification will be a binary classifier
of "suitable" or "not suitable". It is envisaged that some fuzzy categorisation of the candidate will also be
possible which will allow for a human to override a decision on the candidate who is borderline. The opportunity
to use some form of transfer or cluster learning would remove the need to train all roles separately but the initial
design will not cater for this scenario.

\subsubsection{Objective 1. To classify a candidate into a small number of groups that give a sliding suitability score.}

This objective corresponds to research question 1. To the best of my knowledge, through my literature review, no research exists that addresses the classification of closed-ended medical questionnaire's using fuzzy association rule mining. Furthermore, my literature review shows that no work has taken the predicted rule parameters from such mining and applied neural networks to fine tune the results.

\noindent
Fuzzy classification can be considered as two distinct major steps:

\noindent
\textbf{Step 1}: Create our distinct linguistic definitions to be able to describe a given domain feature in a fuzzy or non crisp manner.

In this step we introduce similarity matrix which allow us to compare two linguistic terms in a fuzzy manner. For instance suppose we had a health ranking feature (Table~\ref{tab:SimilarityMatrix}) By using this table we could say that a health ranking value of \textit{Very Poor} had a similarity of 0.5 when compared to \textit{Poor}.

\input{Table/Tbl_similarrty_matrix.tex}

Membership functions are also defined in this step. A membership function best describes the degree of truth in fuzzy logic. As an example if we were considering \textit{diastolic blood pressure} as a feature then this could be described with the membership functions shown in Figure \ref{fig:diastolic}.

\input{fig/Fig_diastolic_function.tex}

\noindent
\textbf{Step 2}: Select the most noteworthy classification rules using the linguistic definitions from step 1 along with any crisp values. This step needs to include a mechanism for handling exponential growth in noteworthy rules as the features in a domain increase.

For traditional crisp association rule mining our feature values maybe boolean or quantitative. The selection process involves finding rules that satisfy a minimum support and confidence value. So if we suppose we have a set of items

\textit{I} = \{$i_1$, $i_2$,...., $i_m$\}

\noindent
and \textit{D} represents the set of all transactions where each transaction \textit{t} is a set of items and \textit{t} $\subseteq$ \textit{I}.
% We introduce a unique identifier \textit{TID} to each transaction and 
Let \textit{A} be a set of arbitrary items in \textit{I}. A transaction \textit{t} contains \textit{A} $\iff$ \textit{A} $\subseteq$ \textit{I}. Each association rule is of the form \textit{A} $\Rightarrow$ \textit{B}, where \textit{A} $\subset$ \textit{I}, \textit{B} $\subset$ \textit{I}, and \textit{A} $\cap$ \textit{B} = $\phi$.
We can now define support (\textit{Sup}) as the percentage of transactions that contain both \textit{A} and \textit{B}

% See http://www.lsv.fr/~markey/LaTeX/doc/Mathmode.pdf
% https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols

\begin{align}
    \textit{Sup}(\textit{A} \Rightarrow \textit{B}) = \frac{\mid \textit{t}_A \cap \textit{t}_B \mid}{\mid \textit{D} \mid}
    % \textit{Sup}(\textit{A} $\Rightarrow$ \textit{B}) = $\frac{\textit{t} \cap}{\textit{D}}$
\end{align}

% \begin{align}
%     \label{eq:centered}2f(x) & = \int\frac{1}{x^2}\,\mathrm{d}x3
% \end{align}

\noindent
and confidence \textit{Conf} as the percentage of transactions in \textit{D} containing \textit{A} that also contain \textit{B}.

\begin{align}
    \textit{Conf} = \frac{\textit{Sup}(\textit{A} \Rightarrow \textit{B})}{\textit{Sup}(\textit{A})} = \frac{\mid \textit{t}_A \cap \textit{t}_B \mid}{\mid \textit{t}_B \mid}
    % \textit{Sup}(\textit{A} $\Rightarrow$ \textit{B}) = $\frac{\textit{t} \cap}{\textit{D}}$
\end{align}

\noindent
Now typically in a non fuzzy approach we would introduce the Apriori algorithm which uses the
definitions for \textit{Sup} and \textit{Conf} to perform a two step procedure where itemsets are generated and tested against \textit{Sup}. The steps are:

\noindent
\textbf{Step 2.1}: set \textit{k} = 1; \textit{largeItemSet} = \textit{empty}

\noindent
\textbf{Step 2.2}: find all \textit{candidateItemsets} of size \textit{k} from original dataset

\noindent
\textbf{Step 2.3}: if a \textit{candidateItemset} transaction count is greater than \textit{Sup} then it is a frequent itemset and added to the \textit{largeItemSet}.

\noindent
\textbf{Step 2.4}: set \textit{k} = \textit{k} + 1

\noindent
\textbf{Step 2.5}: find all \textit{candidateItemsets} of size \textit{k} from \textit{largeItemSet}

\noindent
\textbf{Step 2.6}: if a \textit{candidateItemset} transaction count is greater than \textit{Sup} then it is a frequent itemset and added to the \textit{largeItemSet}.

\noindent
\textbf{Step 2.7}: if \textit{k} $\leq$ \textit{numberOfFeatures} go to Step 2.4

\noindent
\textbf{Step 2.8}: output all \textit{largeItemSet} combinations that have a confidence value larger than \textit{Conf}

\noindent
Our questionnaire data however does not only contain quantitative data and so we need to define support and confidence formula for our linguistic answers.


To date we have implemented a proof of concept of this technique that replicates the findings of Chen et al.(2008~\cite{chen2008mining}, 2009~\cite{chen2009mining}).
The solution is written in Python and uses Neo4j as a datastore, through using a more dynamic approach the implementation of our research goals should be faster to achieve.


\subsubsection{Objective 2. To define a mechanism whereby results of physical medical assessments are fed back into the system for a better predictor.}

This objective corresponds to research question 2. The physical medical assessment is outside the core architectural boundaries of the current system. There are many factors to this including geographic remoteness or the use of third party independent assessors. In order for any selected candidate to be both accurately determined and done so in a timely fashion it is vitally important that results of physical assessments are fed back into the system promptly. In fulfilling this the research will not only complete a design criteria but will also help to fill another literature gap. Chen et al. (2009~\cite{chen2009mining}) concluded that their design proved to be slow as the fuzzy membership routines were static. In feeding back this assessment information from clinical experts into the system the membership functions themselves will be altered over time and in doing so become dynamic.

To better explain this, imagine we are dealing with a crisp set, a membership function is either \textit{True} or \textit{False}, for example someone is either born in Australia or not, there is no middle ground. What about however if we wanted to specify if a person is \textit{Healthy} this is far less rigid. It could be decided on a number of factors such as they (i) can perform a set number of exercises without becoming breathless; (ii) have not visited a doctor in X years; (iii) do not consume cigarettes, alcohol or other stimulants. It is for this type of attribute that membership functions are required, they will assign a value in the range of 0 (\textit{False}) \& 1 (\textit{True}) to establish in this case how \textit{Healthy} someone is from assigning weightings to the factors (i),(ii) \& (iii) previously mentioned.

Marasini et al. (2016~\cite{marasini2016intuitionistic}) state that there are four main approaches that can be used
to define a membership function:

\begin{itemize}
    \item a "best" mathematical function
    \item a probabilistic point of view
    \item decision-theoretic approach
    \item axiomatic measurement theory
\end{itemize}

The idea of intuitionistic fuzzy sets in questionnaire analysis has been covered extensively within the work of Marasini et al. (2016~\cite{marasini2016intuitionistic}).

Kostikova et al. (2016~\cite{kostikova2016expert}) cover how dynamic membership functions maybe achieved.


\subsubsection{Objective 3. To build an anomaly detection routine to predict a list of candidates of concern.}

This objective corresponds to research question 3. The literature review has demonstrated very little research that has catered for questionnaire responses to all of Marshall's (2005~\cite{marshall2005purpose}) five data types. The main objective of this research is to use fuzzy association rule mining to improve on this situation. This objective however will endeavour to apply a number of the state of the art algorithms used in machine learning to discover rare conditions within the candidates. In completing this objective the very small pool of anomaly detection that has been applied to closed-ended questionnaire data will be enhanced. Once these anomalies are detected the research will verify if removal of such candidates leads to a better predictor of the system as a whole.

%https://au.mathworks.com/help/deeplearning/ug/create-simple-deep-learning-network-for-classification.html

We have to date implemented an anomaly detection technique originally in Octave and later ported into MATLAB. The implemented work uses a current production system that has a lot of data but lacks some integrity constraints to ensure that data is always correct.
A similar approach will be undertaken with the system currently being developed once sufficient data becomes available.

\par
\noindent
The approach can be broken down into the following steps:

\textbf{Step 1:} Select features that will be used to base our decision on whether a candidate maybe considered anomalous. Our industry partner has initially chosen BMI Banding (Table~\ref{tab:BMIBanding}), daily consumption of cigarettes, age and sex. These features should be open to change as the project evolves.

\input{Table/Tbl_bmi_banding.tex}

\textbf{Step 2:} Load, explore and clean data. The data cleansing part of this step is particularly important due to the integrity issues of the current system. The implementation removes data with poor integrity. It is at this step the distribution of a feature will be examined and all features will have their values normalised. Normalisation prevents features with a large range swamping those that do not.

\textbf{Step 3:} Define the network architecture. Whilst using the data solely from the original system it was decided not to use any type of Deep Neural Network. At this point in the research the decision was made to use both the Linear regression (Seber \& Lee, 2012~\cite{seber2012linear}) and SVM (Noble, 2006~\cite{noble2006support}) algorithms as having the ability to exclude rare occurrences was considered more important than the accuracy of those decisions. Moving forward it is the intention of the research to introduce a Deep Neural Network architecture.

% can use the following to decide upon algorithms to use

\textbf{Step 4:} Specify training options.

\textbf{Step 5:} Train the network.

The current system although lacking integrity does contain sufficient data for a classic split of the data into training, cross validation and test data. When the new system comes on line a less rigid split will need to be used involving some kind of k-fold cross validation to cater for the initial lack of data. To date some work has been done using a leave-one-out technique to gain familiarity with the approach.

\textbf{Step 6:} Predict the labels of new data and calculate the classification accuracy. Both linear regression and SVM struggled to correctly classify anomalous candidates. In total from a test population of 2834 there were actually 20 candidates rejected however our solution predicted none of these. Upon closer examination of these 20 however it became apparent that these candidates had been rejected for reasons outside of the questionnaire. For example one candidate was rejected after the assessor discovered that they had only one arm which was a question not asked through the particular job questionnaire but the job had required both arms. Other candidates were rejected for reasons that the company gave to the assessor directly and not through the questionnaire process. These findings will be fed back into giving the next iteration of the system more data integrity.


\subsubsection{Objective 4. To build a model whereby assessments maybe compared along a timeline so that assessments taken multiple times maybe analysed.}

This objective corresponds to research question 4. Chen et al. (2009~\cite{chen2009mining}) had demonstrated the inability of their research to analyse associations between questionnaire's over time. By addressing this gap this research will also benefit the industry partner's goal of comparing candidate's assessments over time. This will answer a broader question of whether a given candidate may prove unsuitable for any role or simply an individual role. One further goal of the industry partner is to allow unsuccessful candidates to be given suggestions of possible alternate rolls that they would be suitable for. This objective goes a long way to achieving this goal.

As different job roles will require the use of varying sets of questions it is paramount that we are able to in some way compare the answers for different roles in order that we may recommend an unsuccessful candidate for a more suitable role. Krusaa et al. (\cite{krusaatransfer}) propose a \textit{Questionnaire-Based Efficient Adaptive Transfer Neural Network}, QEATNN that learns to transfer knowledge between a social media and questionnaire domain. We will adapt this approach of transferring knowledge between disparate domains and instead use differing questionnaire domains.

The basic steps are:

\noindent
\textbf{Step 1:} Select representative questions from each questionnaire that are able to be transferred

\noindent
\textbf{Step 2:} Apply the work of Shi et al. (2017, \cite{shi2017local}) to create a model using Local Representative-Based
Matrix Factorisation, LRMF that splits the representative questions into global (global attributes) and local (personal attributes) representatives.

\noindent
\textbf{Step 3:} Deduce whether sufficient local representatives of an unsuccessful candidate from one questionnaire domain match those of a successful candidate from another domain and if so recommend this candidate for the alternate role.

\subsection{Demonstrate the artefact can be used to help solve the focal problem}

Figure~\ref{fig:specificjobrequirement} demonstrates a more specific job requirement for lifting heavy weights from floor
to waist. It has been partially completed for the purpose of explanation only and does not represent a genuine scenario.
Of particular note are the edges for heavy weight,
hip and flexor. It is on these edges that properties will initially be attached such as the value for the actual weight
considered "heavy" by an expert in the field. As our classifier begins to acquire data it is these properties that will
dynamically be altered in order to solve our core problem of classifying a candidate. An added bonus of the classifier
will be the ability to suggest alternate candidate roles to an unsuccessful candidate that they may be suitable for.

\input{fig/Fig_specific_job.tex}


\subsection{Evaluate how well the artefact solves the focal problem}

Objective 5 of Section~\ref{sec:disObjectives} discusses evaluation of the developed artefacts and in so doing satisfies
this requirement of Peffers et al. described in the Methodology Section \ref{sec:disMethodology}.

\subsubsection{Objective 5. To evaluate the developed artefacts from the previous objectives.}

This objective corresponds to research question 5. For our classification objective there are a number of typical tests to indicate the correctness of a classification and these include amongst other measures confusion matrix, area under the ROC curve and F1 score.

%Evaluation should state what success looks like, for instance, objective 1 may be successful if we can flag 60\% of candidates of concern. Or our prototype should show improved learning through assessor feedback.

% https://towardsdatascience.com/how-to-measure-the-goodness-of-a-regression-model-60c7f87614ce
%https://medium.com/analytics-vidhya/evaluation-metrics-in-machine-learning-models-using-python-fb6199450fba
% https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9


Briefly if we are describing a binary classification such as the candidate is "suitable" or "not suitable" we can plot a confusion matrix of the form shown in Fig~\ref{fig:ConfusionMatrix}. From this matrix we can calculate attributes such as accuracy, precision and recall to decide whether our results have been noteworthy. The reader may at this point believe that to be noteworthy we should simply strive for the highest accuracy in our classification but that may not always be the case. For instance we may gain an accuracy of over 98\% but if we ultimately select a candidate who would have been rejected if given a physical assessment then this could present an unforeseen cost to the client and also a loss of faith in the predictor. This then brings us to the other two attributes precision and recall which represent the ratio of true positives in the model to the predicted positives and actual positives respectively. Precision should be closely watched when the cost of a false positive is high and recall when the cost of a false negative is high.

Initially all of our candidates will have a medical assessment and so the cost of a false positive will not truly be of concern. That does not however mean that our success of a classification for this research should only look at accuracy as eventually the medical assessor should not need to be called upon for every situation. It does however offer some flexibility in deciding the success criteria at this stage of the research.

\begin{figure}[h]
    \caption{Confusion matrix} \label{fig:ConfusionMatrix}
    \begin{tabular}{|l|l|l|}
        \hline
                                          & \textbf{Suitable (Actual)} & \textbf{Not Suitable (Actual)} \\ \hline
        \textbf{Suitable (Predicted)}     & TP (True Positive)         & FP (False Positive)            \\ \hline
        \textbf{Not Suitable (Predicted)} & FN (False Negative)        & TN (True Negative)             \\ \hline
    \end{tabular}

    % https://www.latex-tutorial.com/tutorials/amsmath/
    \begin{align*}
        accuracy  & = \frac{(TP+TN)}{(TP+TN+FP+FN)} \\
        precision & = \frac{TP}{(TP+FP)}            \\
        recall    & = \frac{TP}{(TP+FN)}            \\
    \end{align*}

\end{figure}


% https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/
\noindent
Fawcett (2006~\cite{fawcett2006introduction}) explains the intricacies of an ROC graph, an example of which, is shown in Fig~\ref{fig:ROCCurve}. It is a very visual means by which the correctness of a classifier can be judged by varying the threshold. The threshold being a value between 0\% and 100\%, that is used to set the limit to decide upon which class an instance belongs to. In our case, we will classify a candidate as either "suitable" or "not suitable". The graph plots the true positive rate against the false positive rate. For us the true positive rate is the rate in which candidates are correctly identified as "suitable" for the job role in question. A classifier that approaches the top left of the graph is considered a better classifier than one further away. The closer the curve comes to the "random" 45 degree line the less accurate the classifier and this would equate to using a "coin toss" to decide upon the candidates suitability. Although an ROC graph is a very visual tool, to evaluate multiple classifiers the approach used is to take the area under the ROC curve (AUC). Generally, although not always true, a high AUC score is a better predictor than one that is lower. One advantage that an ROC graph has over a confusion matrix is that it does not depend on class distribution and hence is still suitable for evaluating classifiers that contain rare or anomalous values.

\input{Chart_ROC}

The final success indicator that we will employ is the F1 score. This score is based on the precision and recall values mentioned in Fig~\ref{fig:ConfusionMatrix}

\begin{align*}
    F1 & = 2 * \frac{Precision*Recall}{Precision+Recall} \\
\end{align*}

It is used when a balance is required between precision and recall. The reason for the F1 score over using straight accuracy is that accuracy is affected highly by true negatives which are not often a focus in business problems. False negatives and positives usually claim the majority of the focus as they are responsible for most of the cost involved in incorrect classification. Thus the F1 score seeks to find balance between precision and recall when there is an uneven class distribution.


\subsection{Communicate the outcomes of the research}


Amongst the outcomes of this research will be the development of a number of novel algorithms to be incorporated into a commercial software product. It is the algorithms that are developed during the design phase that will satisfy the artefact requirement of DSR. The stakeholder community will initially involve the industry partner of the university but will ultimately be useful to anyone dealing with the problem of classifying the answers to closed survey/questionnaire data.
\par
Through progressing the research to completion the communication of the outcomes will satisfy the industry partner. The wider research
community will become aware of the outcomes through publishing one or two papers at recognised conferences.



