%\textbf{Objective = Deliverable + Criteria for success. Ideally these should contain your keywords as well.}

%\textit{1.I will deliver a theoretical model of RF-optical EIT2.I will deliver a proof of concept experimental verification of RF-optical EIT3.I will deliver a detailed examination of optical polarization effects in RF-optical EIT}

\noindent
This study will have 5 objectives:
%%%The first objective will use a current production system that has a lot of data but lacks some integrity constraints to ensure that data is always correct. The next 3 objectives will use a system that is being developed in parallel with this study but suffers from not having real data but does address many of the concerns of the current system.

Objective 1. To classify a candidate into a small number of groups that give a sliding suitability score.

%%%This objective corresponds to research question 1. To the best of my knowledge, through my literature review, no research exists that addresses the classification of closed-ended medical questionnaire's using fuzzy association rule mining. Furthermore, my literature review shows that no work has taken the predicted rule parameters from such mining and applied neural networks to fine tune the results.

Objective 2. To define a mechanism whereby results of physical medical assessments are fed back into the system for a better predictor.

%%%This objective corresponds to research question 2. The physical medical assessment is outside the core architectural boundaries of the current system. There are many factors to this including geographic remoteness or the use of third party independent assessors. In order for any selected candidate to be both accurately determined and done so in a timely fashion it is vitally important that results of physical assessments are fed back into the system promptly. In fulfilling this the research will not only complete a design criteria but will also help to fill another literature gap. Chen et al. (2009~\cite{chen2009mining}) concluded that their design proved to be slow as the fuzzy membership routines were static. In feeding back this assessment information into the system the research will use the data to deduce dynamic membership routines.

Objective 3. To build an anomaly detection routine to predict a list of candidates of concern.

%%%This objective corresponds to research question 3. The literature review has demonstrated very little research that has catered for questionnaire responses to all of Marshall's (2005~\cite{marshall2005purpose}) five data types. The main objective of this research is to use fuzzy association rule mining to improve on this situation. This objective however will endeavour to apply a number of the state of the art algorithms used in machine learning to discover rare conditions within the candidates. In completing this objective the very small pool of anomaly detection that has been applied to closed-ended questionnaire data will be enhanced. Once these anomalies are detected the research will verify if removal of such candidates leads to a better predictor of the system as a whole.

Objective 4. To build a model whereby assessments maybe compared along a timeline so that assessments taken multiple times maybe analysed.

%%%This objective corresponds to research question 4. Chen et al. (2009~\cite{chen2009mining}) had demonstrated the inability of their research to analyse associations between questionnaire's over time. By addressing this gap this research will also benefit the industry partner's goal of comparing candidate's assessments over time. This will answer a broader question of whether a given candidate may prove unsuitable for any role or simply an individual role. One further goal of the industry partner is to allow unsuccessful candidates to be given suggestions of possible alternate rolls that they would be suitable for. This objective goes a long way to achieving this goal.



%was the first to use fuzzy association rules on questionnaire data and in so doing was able to handle all of the data types simultaneously. They achieved this through no longer considering answers as true/false but as partial truths thus any answer that is of a linguistic type can considered alongside a non fuzzy type. Although the research had some success the authors do concede some shortcomings in the approach. The first being the use of static membership functions that are defined ahead of time that create roadblocks in the process.This work will consider a dynamic membership function which will derive the function from the data. Another being a mechanism for analysing associations between questionnaire's over time. 

Objective 5. To evaluate the developed artefacts from the previous objectives.

%%%This objective corresponds to research question 5. For our classification objective there are a number of typical tests to indicate the correctness of a classification and these include amongst other measures confusion matrix, area under the ROC curve and F1 score.

%Evaluation should state what success looks like, for instance, objective 1 may be successful if we can flag 60\% of candidates of concern. Or our prototype should show improved learning through assessor feedback.

% https://towardsdatascience.com/how-to-measure-the-goodness-of-a-regression-model-60c7f87614ce
%https://medium.com/analytics-vidhya/evaluation-metrics-in-machine-learning-models-using-python-fb6199450fba
% https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9


%%%Briefly if we are describing a binary classification such as the candidate is "suitable" or "not suitable" we can plot a confusion matrix of the form shown in Fig~\ref{fig:ConfusionMatrix}. From this matrix we can calculate attributes such as accuracy, precision and recall to decide whether our results have been noteworthy. The reader may at this point believe that to be noteworthy we should simply strive for the highest accuracy in our classification but that may not always be the case. For instance we may gain an accuracy of over 98\% but if we ultimately select a candidate who would have been rejected if given a physical assessment then this could present an unforeseen cost to the client and also a loss of faith in the predictor. This then brings us to the other two attributes precision and recall which represent the ratio of true positives in the model to the predicted positives and actual positives respectively. Precision should be closely watched when the cost of a false positive is high and recall when a the cost of a false negative is high.

%%%Initially all of our candidates will have a medical assessment and so the cost of a false positive will not truly be of concern. That does not however mean that our success of a classification for this research should only look at accuracy as eventually the medical assessor should not need to be called upon for every situation. It does however offer some flexibility in deciding the success criteria at this stage of the research.

%%%\begin{figure}[h]
%%%  \caption{Confusion matrix} \label{fig:ConfusionMatrix}
%%%  \begin{tabular}{|l|l|l|}
%%%    \hline
%%%                                      & \textbf{Suitable (Actual)} & \textbf{Not Suitable (Actual)} \\ \hline
%%%    \textbf{Suitable (Predicted)}     & TP (True Positive)         & FP (False Positive)            \\ \hline
%%%    \textbf{Not Suitable (Predicted)} & FN (False Negative)        & TN (True Negative)             \\ \hline
%%%  \end{tabular}

% https://www.latex-tutorial.com/tutorials/amsmath/
%%%  \begin{align*}
%%%    accuracy  & = \frac{(TP+TN)}{(TP+TN+FP+FN)} \\
%%%    precision & = \frac{TP}{(TP+FP)}            \\
%%%    recall    & = \frac{TP}{(TP+FN)}            \\
%%%  \end{align*}

%%%\end{figure}


% https://www.displayr.com/what-is-a-roc-curve-how-to-interpret-it/
%%%\noindent
%%%Fawcett (2006~\cite{fawcett2006introduction}) explains the intricacies of an ROC graph, an example of which, is shown in Fig~\ref{fig:ROCCurve}. It is a very visual means by which the correctness of a classifier can be judged by varying the threshold. The threshold being a value between 0\% and 100\%, that is used to set the limit to decide upon which class an instance belongs to. In our case, we will classify a candidate as either "suitable" or "not suitable". The graph plots the true positive rate against the false positive rate. For us the true positive rate is the rate in which candidates are correctly identified as "suitable" for the job role in question. A classifier that approaches the top left of the graph is considered a better classifier than one further away. The closer the curve comes to the "random" 45 degree line the less accurate the classifier and this would equate to using a "coin toss" to decide upon the candidates suitability. Although an ROC graph is a very visual tool, to evaluate multiple classifiers the approach used is to take the area under the ROC curve (AUC). Generally, although not always true, a high AUC score is a better predictor than one that is lower. One advantage that an ROC graph has over a confusion matrix is that it does not depend on class distribution and hence is still suitable for evaluating classifiers that contain rare or anomalous values.

%%%\input{Chart_ROC}

%%%The final success indicator that we will employ is the F1 score. This score is based on the precision and recall values mentioned in Fig~\ref{fig:ConfusionMatrix}

%%%\begin{align*}
%%%  F1 & = 2 * \frac{Precision*Recall}{Precision+Recall} \\
%%%\end{align*}

%%%It is used when a balance is required between precision and recall. The reason for the F1 score over using straight accuracy is that accuracy is affected highly by true negatives which are not often a focus in business problems. False negatives and positives usually claim the majority of the focus as they are responsible for most of the cost involved in incorrect classification. Thus the F1 score seeks to find balance between precision and recall when there is an uneven class distribution.


%TP/ (TP+FN)​
%TP/ (TP+FP​)
%(TP+TN) / (TP+TN+FP+FN)




%\par
%\noindent
%Alternative objectives

%\textbf{Objective 1. To apply fuzzy logic association rules to a closed-ended %medical questionnaire in order to predict a candidates suitability}

%\textbf{Objective 2. To apply neural network algorithms to the results of objective 1 in order to improve on our predictions}

%\textbf{Objective 3. To apply neural network algorithms to a closed-ended medical questionnaire in order to predict a candidate's suitability}

%\textbf{Objective 4. To evaluate the developed artefacts from the previous objectives.}

% \noindent
% \textit{Each objective maybe linked to the publication of a research paper}

\noindent
These objectives are further elaborated upon in Section~\ref{sec:disResearchPlan}
